{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6eb47bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Setup ---\n",
      "--- Loading Data ---\n",
      "Using CF Algorithm: SVD with params: {'n_factors': 100, 'n_epochs': 30, 'lr_all': 0.007, 'reg_all': 0.04}\n",
      "\n",
      "--- Splitting Data for Surprise CF Evaluation (Train/Test) ---\n",
      "Pandas train ratings for ranking eval: 80668\n",
      "Pandas test ratings for ranking eval: 20168\n",
      "Evaluation recommendation functions adapted.\n",
      "\n",
      "--- Collaborative Filtering Evaluation (RMSE/MAE) with Tuned Surprise ---\n",
      "RMSE: 0.8692\n",
      "MAE:  0.6658\n",
      "\n",
      "Ranking metrics helper defined.\n",
      "\n",
      "--- Content-Based & Hybrid Ranking Evaluation (Precision@10, Recall@10) ---\n",
      "Avg Content Precision@10: 0.0080, Recall@10: 0.0081 (on 50 users)\n",
      "Avg Hybrid Precision@10: 0.0860, Recall@10: 0.0890 (on 50 users)\n",
      "\n",
      "\n",
      "--- Evaluation Summary (with Tuned Surprise CF) ---\n",
      "Collaborative Filtering (Surprise SVD) RMSE: 0.8692\n",
      "Collaborative Filtering (Surprise SVD) MAE: 0.6658\n",
      "Content-Based Avg Precision@10: 0.0080\n",
      "Content-Based Avg Recall@10: 0.0081\n",
      "Hybrid Model Avg Precision@10: 0.0860\n",
      "Hybrid Model Avg Recall@10: 0.0890\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.model_selection import train_test_split # Using Surprise's split for CF part\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.decomposition import TruncatedSVD # No longer needed for CF evaluation\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- Surprise Library Imports ---\n",
    "from surprise import Dataset, Reader, SVD, SVDpp # Or other algos\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "# --- End Surprise Library Imports ---\n",
    "\n",
    "print(\"--- Evaluation Setup ---\")\n",
    "DATA_PATH = '../data/'\n",
    "MODELS_PATH = '../models/' # If loading pre-trained models\n",
    "\n",
    "TOP_K = 10\n",
    "MIN_RATING_THRESHOLD_CONTENT_PROFILE = 4.0\n",
    "MIN_RATING_THRESHOLD_RELEVANT_TEST = 4.0\n",
    "\n",
    "# Cell 2: Load Data\n",
    "print(\"--- Loading Data ---\")\n",
    "movies_df = pd.read_csv(DATA_PATH + 'movies_processed.csv')\n",
    "ratings_df = pd.read_csv(DATA_PATH + 'ratings.csv')\n",
    "cosine_sim_content = np.load(DATA_PATH + 'cosine_similarity_content.npy')\n",
    "indices_map_movieId_to_df_idx = pd.Series(movies_df.index, index=movies_df['movieId']).drop_duplicates()\n",
    "\n",
    "# --- !!! USE YOUR ACTUAL TUNED PARAMETERS HERE !!! ---\n",
    "# These should match what you found from GridSearchCV in 03_...ipynb\n",
    "BEST_CF_PARAMS_FROM_TUNING = {'n_factors': 100, 'n_epochs': 30, 'lr_all': 0.007, 'reg_all': 0.04}\n",
    "CHOSEN_CF_ALGORITHM_EVAL = 'SVD' # Or 'SVDpp'\n",
    "print(f\"Using CF Algorithm: {CHOSEN_CF_ALGORITHM_EVAL} with params: {BEST_CF_PARAMS_FROM_TUNING}\")\n",
    "# --- END TUNED PARAMETERS ---\n",
    "\n",
    "# Cell 3: Data Splitting for Surprise\n",
    "print(\"\\n--- Splitting Data for Surprise CF Evaluation (Train/Test) ---\")\n",
    "reader_eval = Reader(rating_scale=(ratings_df['rating'].min(), ratings_df['rating'].max()))\n",
    "data_eval = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader_eval)\n",
    "trainset_eval, testset_eval = surprise_train_test_split(data_eval, test_size=0.2, random_state=42)\n",
    "\n",
    "# For content-based and hybrid ranking evaluation, we need pandas DFs for train/test ratings\n",
    "# This split should be consistent with Surprise's split if possible, but for simplicity\n",
    "# we can make a new pandas split. For true consistency, one would map Surprise testset back to pandas.\n",
    "# Here, we'll use a simple pandas split for the ranking part.\n",
    "from sklearn.model_selection import train_test_split as pd_train_test_split\n",
    "pd_train_ratings_df, pd_test_ratings_df = pd_train_test_split(\n",
    "    ratings_df, test_size=0.2, random_state=42, stratify=ratings_df['userId']\n",
    ")\n",
    "print(f\"Pandas train ratings for ranking eval: {len(pd_train_ratings_df)}\")\n",
    "print(f\"Pandas test ratings for ranking eval: {len(pd_test_ratings_df)}\")\n",
    "\n",
    "\n",
    "# Cell 4: Re-define/Adapt Recommendation Functions (from 03_...ipynb, now for evaluation context)\n",
    "# (These functions - train_collaborative_filtering_surprise_eval, get_collaborative_recommendations_surprise_eval,\n",
    "#  get_content_recommendations_for_user_eval, get_hybrid_recommendations_eval - would be defined here.\n",
    "#  They are similar to those in the previous 04_evaluation.ipynb skeleton I gave, but ensure\n",
    "#  CF parts use Surprise and are trained on `trainset_eval` or `pd_train_ratings_df` as appropriate.)\n",
    "\n",
    "# --- CF Evaluation Training Function (using Surprise) ---\n",
    "def train_cf_surprise_for_eval(local_trainset, algo_choice='SVD', best_params=None, random_state=42):\n",
    "    if best_params is None: # Fallback defaults\n",
    "        best_params = {'n_factors': 100, 'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.02}\n",
    "    \n",
    "    if algo_choice == 'SVDpp':\n",
    "        algo = SVDpp(**best_params, random_state=random_state, verbose=False, cache_ratings=True)\n",
    "    else:\n",
    "        algo = SVD(**best_params, biased=True, random_state=random_state, verbose=False)\n",
    "    algo.fit(local_trainset)\n",
    "    return algo\n",
    "\n",
    "# --- CF Recommendation Function for Ranking (using Surprise) ---\n",
    "def get_cf_recs_surprise_for_eval(user_id, surprise_algo, local_trainset, movies_df_cf, \n",
    "                                  all_ratings_df_for_exclusion, top_n=10):\n",
    "    # Similar to get_collaborative_recommendations_surprise from 03_...\n",
    "    # Ensures it uses the local_trainset from the evaluation split\n",
    "    all_movie_raw_ids = [local_trainset.to_raw_iid(inner_id) for inner_id in local_trainset.all_items()]\n",
    "    rated_movie_ids = all_ratings_df_for_exclusion[all_ratings_df_for_exclusion['userId'] == user_id]['movieId'].unique().tolist()\n",
    "    recommendations = []\n",
    "    try: _ = local_trainset.to_inner_uid(user_id)\n",
    "    except ValueError: pass # User not in trainset, predict will use global avg\n",
    "\n",
    "    for movie_id in all_movie_raw_ids:\n",
    "        if movie_id not in rated_movie_ids:\n",
    "            prediction = surprise_algo.predict(uid=user_id, iid=movie_id)\n",
    "            movie_detail = movies_df_cf[movies_df_cf['movieId'] == movie_id]\n",
    "            if not movie_detail.empty:\n",
    "                recommendations.append({\n",
    "                    'movieId': movie_id, 'title_clean': movie_detail['title_clean'].iloc[0],\n",
    "                    'predicted_collaborative_score': prediction.est,\n",
    "                    'genres_str': movie_detail.get('genres_str', pd.Series([\"\"])).iloc[0]\n",
    "                })\n",
    "    recs_df = pd.DataFrame(recommendations)\n",
    "    if not recs_df.empty:\n",
    "        recs_df = recs_df.sort_values(by='predicted_collaborative_score', ascending=False).head(top_n)\n",
    "    return recs_df\n",
    "\n",
    "# --- Content-Based (Same as your corrected version) ---\n",
    "def get_content_recs_for_eval(user_id, ratings_for_profile_df, movies_df_c, cosine_sim_m, \n",
    "                               local_indices_map, top_n=10, min_rating_thresh=4.0):\n",
    "    # This is your get_content_recommendations_for_user function\n",
    "    # Ensure it uses ratings_for_profile_df (which should be pd_train_ratings_df for evaluation)\n",
    "    # and excludes all items from ratings_for_profile_df from recommendations.\n",
    "    user_ratings = ratings_for_profile_df[(ratings_for_profile_df['userId'] == user_id) & (ratings_for_profile_df['rating'] >= min_rating_thresh)]\n",
    "    if user_ratings.empty: return pd.DataFrame()\n",
    "    liked_movie_ids = user_ratings['movieId'].tolist()\n",
    "    liked_movie_indices = [local_indices_map[mid] for mid in liked_movie_ids if mid in local_indices_map and 0 <= local_indices_map[mid] < cosine_sim_m.shape[0]]\n",
    "    if not liked_movie_indices: return pd.DataFrame()\n",
    "    try: profile_sim_vec = np.mean(cosine_sim_m[liked_movie_indices, :], axis=0)\n",
    "    except: return pd.DataFrame()\n",
    "    sim_scores_s = pd.Series(profile_sim_vec, index=movies_df_c.index).sort_values(ascending=False)\n",
    "    \n",
    "    # Exclude ALL movies the user rated in the profile-building set (pd_train_ratings_df)\n",
    "    rated_in_profile_set = ratings_for_profile_df[ratings_for_profile_df['userId'] == user_id]['movieId'].unique().tolist()\n",
    "\n",
    "    recs = []\n",
    "    for idx, score in sim_scores_s.items():\n",
    "        if len(recs) >= top_n: break\n",
    "        info = movies_df_c.loc[idx]\n",
    "        mid_rec = info['movieId']\n",
    "        if mid_rec not in rated_in_profile_set: # Crucial exclusion\n",
    "            genres = info.get('genres_str', \"\")\n",
    "            if not isinstance(genres, str) or pd.isna(genres): genres = \"\"\n",
    "            recs.append({'movieId': mid_rec, 'title_clean': info['title_clean'], 'predicted_content_score': score, 'genres_str': genres})\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "# --- Hybrid (Adapted to use eval functions) ---\n",
    "def get_hybrid_recs_for_eval(user_id, cf_algo_eval, cf_trainset_eval, cosine_sim_eval, \n",
    "                             indices_map_eval, pd_ratings_train_for_profile, movies_df_eval, \n",
    "                             top_n=10, collab_w=0.5, content_w=0.5, min_rating_thresh_c_prof=4.0):\n",
    "    num_init = top_n * 3\n",
    "    # Use pd_ratings_train_for_profile for CF exclusion logic as well\n",
    "    collab_recs = get_cf_recs_surprise_for_eval(user_id, cf_algo_eval, cf_trainset_eval, movies_df_eval, pd_ratings_train_for_profile, top_n=num_init)\n",
    "    content_recs = get_content_recs_for_eval(user_id, pd_ratings_train_for_profile, movies_df_eval, cosine_sim_eval, indices_map_eval, top_n=num_init, min_rating_thresh=min_rating_thresh_c_prof)\n",
    "    \n",
    "    # ... (The rest of the hybrid merging logic as in 03_... or previous 04_... skeleton) ...\n",
    "    # This part needs careful review to ensure normalized_score columns are handled correctly if one is empty.\n",
    "    no_content = content_recs.empty\n",
    "    no_collab = collab_recs.empty\n",
    "    if no_content and no_collab: return pd.DataFrame()\n",
    "    scaler = MinMaxScaler()\n",
    "    if not no_content and 'predicted_content_score' in content_recs: content_recs['normalized_score'] = scaler.fit_transform(content_recs[['predicted_content_score']]) if content_recs['predicted_content_score'].nunique()>1 else 0.5\n",
    "    else: content_recs = pd.DataFrame(columns=list(content_recs.columns)+['normalized_score']); no_content=True\n",
    "    if not no_collab and 'predicted_collaborative_score' in collab_recs: collab_recs['normalized_score'] = scaler.fit_transform(collab_recs[['predicted_collaborative_score']]) if collab_recs['predicted_collaborative_score'].nunique()>1 else 0.5\n",
    "    else: collab_recs = pd.DataFrame(columns=list(collab_recs.columns)+['normalized_score']); no_collab=True\n",
    "    \n",
    "    if no_content and not no_collab: return collab_recs.head(top_n) # Simplified return for brevity\n",
    "    if no_collab and not no_content: return content_recs.head(top_n)\n",
    "    if no_collab and no_content: return pd.DataFrame()\n",
    "\n",
    "    merged = pd.merge(content_recs[['movieId', 'title_clean', 'genres_str', 'normalized_score']], collab_recs[['movieId', 'normalized_score']], on='movieId', how='outer', suffixes=('_c', '_cf')).fillna(0)\n",
    "    merged['hybrid_score'] = (content_w * merged['normalized_score_c']) + (collab_w * merged['normalized_score_cf'])\n",
    "    if 'title_clean_c' in merged.columns: merged['title_clean'] = merged['title_clean_c'] # Consolidate\n",
    "    final = merged.sort_values('hybrid_score', ascending=False).drop_duplicates('movieId').head(top_n)\n",
    "    return final[['movieId', 'title_clean', 'hybrid_score', 'genres_str']]\n",
    "\n",
    "print(\"Evaluation recommendation functions adapted.\")\n",
    "\n",
    "\n",
    "# Cell 5: Collaborative Filtering Evaluation (RMSE/MAE with Surprise)\n",
    "print(\"\\n--- Collaborative Filtering Evaluation (RMSE/MAE) with Tuned Surprise ---\")\n",
    "cf_algo_eval = train_cf_surprise_for_eval(trainset_eval, algo_choice=CHOSEN_CF_ALGORITHM_EVAL, best_params=BEST_CF_PARAMS_FROM_TUNING)\n",
    "predictions_eval = cf_algo_eval.test(testset_eval)\n",
    "rmse_cf_surprise = accuracy.rmse(predictions_eval, verbose=True)\n",
    "mae_cf_surprise = accuracy.mae(predictions_eval, verbose=True)\n",
    "\n",
    "\n",
    "# Cell 6: Ranking Metrics Helper Function\n",
    "def calculate_precision_recall_at_k(recommended_df, relevant_ids, k_val):\n",
    "    if recommended_df.empty or not relevant_ids: return 0.0, 0.0\n",
    "    rec_k_ids = set(recommended_df['movieId'].head(k_val).tolist())\n",
    "    relevant_set = set(relevant_ids)\n",
    "    hits = len(rec_k_ids.intersection(relevant_set))\n",
    "    precision = hits / k_val if k_val > 0 else 0.0\n",
    "    recall = hits / len(relevant_set) if relevant_set else 0.0\n",
    "    return precision, recall\n",
    "print(\"\\nRanking metrics helper defined.\")\n",
    "\n",
    "# Cell 7: Content-Based & Hybrid Evaluation (Ranking)\n",
    "print(f\"\\n--- Content-Based & Hybrid Ranking Evaluation (Precision@{TOP_K}, Recall@{TOP_K}) ---\")\n",
    "content_precisions, content_recalls = [], []\n",
    "hybrid_precisions, hybrid_recalls = [], []\n",
    "\n",
    "# Use a smaller sample for quick testing, increase for final evaluation\n",
    "test_user_ids_for_ranking = random.sample(list(pd_test_ratings_df['userId'].unique()), min(50, pd_test_ratings_df['userId'].nunique())) \n",
    "\n",
    "for user_id_eval in test_user_ids_for_ranking:\n",
    "    relevant_in_test = pd_test_ratings_df[(pd_test_ratings_df['userId'] == user_id_eval) & (pd_test_ratings_df['rating'] >= MIN_RATING_THRESHOLD_RELEVANT_TEST)]['movieId'].tolist()\n",
    "    if not relevant_in_test: continue\n",
    "\n",
    "    # Content-Based\n",
    "    cb_recs = get_content_recs_for_eval(user_id_eval, pd_train_ratings_df, movies_df, cosine_sim_content, indices_map_movieId_to_df_idx, top_n=TOP_K, min_rating_thresh=MIN_RATING_THRESHOLD_CONTENT_PROFILE)\n",
    "    p, r = calculate_precision_recall_at_k(cb_recs, relevant_in_test, TOP_K)\n",
    "    content_precisions.append(p); content_recalls.append(r)\n",
    "\n",
    "    # Hybrid\n",
    "    # For hybrid, CF part uses cf_algo_eval (trained on surprise trainset_eval)\n",
    "    # Content part profile uses pd_train_ratings_df\n",
    "    hybrid_recs = get_hybrid_recs_for_eval(user_id_eval, cf_algo_eval, trainset_eval, cosine_sim_content, indices_map_movieId_to_df_idx, pd_train_ratings_df, movies_df, top_n=TOP_K, collab_w=0.7, content_w=0.3) # Example weights\n",
    "    p_h, r_h = calculate_precision_recall_at_k(hybrid_recs, relevant_in_test, TOP_K)\n",
    "    hybrid_precisions.append(p_h); hybrid_recalls.append(r_h)\n",
    "\n",
    "avg_cb_p = np.mean(content_precisions) if content_precisions else 0\n",
    "avg_cb_r = np.mean(content_recalls) if content_recalls else 0\n",
    "avg_h_p = np.mean(hybrid_precisions) if hybrid_precisions else 0\n",
    "avg_h_r = np.mean(hybrid_recalls) if hybrid_recalls else 0\n",
    "\n",
    "print(f\"Avg Content Precision@{TOP_K}: {avg_cb_p:.4f}, Recall@{TOP_K}: {avg_cb_r:.4f} (on {len(content_precisions)} users)\")\n",
    "print(f\"Avg Hybrid Precision@{TOP_K}: {avg_h_p:.4f}, Recall@{TOP_K}: {avg_h_r:.4f} (on {len(hybrid_precisions)} users)\")\n",
    "\n",
    "\n",
    "# Cell 8: Summary of Results\n",
    "print(\"\\n\\n--- Evaluation Summary (with Tuned Surprise CF) ---\")\n",
    "print(f\"Collaborative Filtering (Surprise {CHOSEN_CF_ALGORITHM_EVAL}) RMSE: {rmse_cf_surprise:.4f}\") # This is a variable from surprise.accuracy\n",
    "print(f\"Collaborative Filtering (Surprise {CHOSEN_CF_ALGORITHM_EVAL}) MAE: {mae_cf_surprise:.4f}\")   # This is a variable from surprise.accuracy\n",
    "print(f\"Content-Based Avg Precision@{TOP_K}: {avg_cb_p:.4f}\")\n",
    "print(f\"Content-Based Avg Recall@{TOP_K}: {avg_cb_r:.4f}\")\n",
    "print(f\"Hybrid Model Avg Precision@{TOP_K}: {avg_h_p:.4f}\")\n",
    "print(f\"Hybrid Model Avg Recall@{TOP_K}: {avg_h_r:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
